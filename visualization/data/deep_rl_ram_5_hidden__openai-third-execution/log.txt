Logging to C:\Users\dalme\AppData\Local\Temp\openai-2018-06-24-18-31-48-616382
--------------------------------------
| % time spent exploring  | 82       |
| episodes                | 100      |
| mean 100 episode reward | 1        |
| steps                   | 17389    |
--------------------------------------
Saving model due to mean reward increase: None -> 1.0
--------------------------------------
| % time spent exploring  | 65       |
| episodes                | 200      |
| mean 100 episode reward | 1        |
| steps                   | 34480    |
--------------------------------------
Saving model due to mean reward increase: 1.0 -> 1.1
Saving model due to mean reward increase: 1.1 -> 1.2
--------------------------------------
| % time spent exploring  | 48       |
| episodes                | 300      |
| mean 100 episode reward | 1.1      |
| steps                   | 52512    |
--------------------------------------
Saving model due to mean reward increase: 1.2 -> 1.4
--------------------------------------
| % time spent exploring  | 29       |
| episodes                | 400      |
| mean 100 episode reward | 1.4      |
| steps                   | 71652    |
--------------------------------------
Saving model due to mean reward increase: 1.4 -> 1.7
--------------------------------------
| % time spent exploring  | 8        |
| episodes                | 500      |
| mean 100 episode reward | 1.6      |
| steps                   | 92755    |
--------------------------------------
Saving model due to mean reward increase: 1.7 -> 1.9
Saving model due to mean reward increase: 1.9 -> 2.3
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 600      |
| mean 100 episode reward | 2.8      |
| steps                   | 119379   |
--------------------------------------
Saving model due to mean reward increase: 2.3 -> 2.7
Saving model due to mean reward increase: 2.7 -> 2.8
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 700      |
| mean 100 episode reward | 2.5      |
| steps                   | 143161   |
--------------------------------------
Saving model due to mean reward increase: 2.8 -> 3.0
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 800      |
| mean 100 episode reward | 2.9      |
| steps                   | 169574   |
--------------------------------------
Saving model due to mean reward increase: 3.0 -> 3.2
Saving model due to mean reward increase: 3.2 -> 3.3
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 900      |
| mean 100 episode reward | 3.3      |
| steps                   | 203489   |
--------------------------------------
Saving model due to mean reward increase: 3.3 -> 3.4
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1000     |
| mean 100 episode reward | 3.7      |
| steps                   | 237233   |
--------------------------------------
Saving model due to mean reward increase: 3.4 -> 3.6
Saving model due to mean reward increase: 3.6 -> 4.0
Saving model due to mean reward increase: 4.0 -> 4.1
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1100     |
| mean 100 episode reward | 3.9      |
| steps                   | 267183   |
--------------------------------------
Saving model due to mean reward increase: 4.1 -> 4.4
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1200     |
| mean 100 episode reward | 4.9      |
| steps                   | 305671   |
--------------------------------------
Saving model due to mean reward increase: 4.4 -> 4.9
Saving model due to mean reward increase: 4.9 -> 5.3
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1300     |
| mean 100 episode reward | 4.7      |
| steps                   | 339197   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1400     |
| mean 100 episode reward | 3        |
| steps                   | 365101   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1500     |
| mean 100 episode reward | 4.1      |
| steps                   | 403274   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1600     |
| mean 100 episode reward | 4.8      |
| steps                   | 436363   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1700     |
| mean 100 episode reward | 4        |
| steps                   | 462392   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1800     |
| mean 100 episode reward | 4.1      |
| steps                   | 498052   |
--------------------------------------
Saving model due to mean reward increase: 5.3 -> 5.5
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 1900     |
| mean 100 episode reward | 5.6      |
| steps                   | 537526   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2000     |
| mean 100 episode reward | 4.6      |
| steps                   | 568489   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2100     |
| mean 100 episode reward | 5.5      |
| steps                   | 606527   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2200     |
| mean 100 episode reward | 4        |
| steps                   | 647714   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2300     |
| mean 100 episode reward | 3.3      |
| steps                   | 709521   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2400     |
| mean 100 episode reward | 4.8      |
| steps                   | 747975   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2500     |
| mean 100 episode reward | 3.6      |
| steps                   | 782305   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2600     |
| mean 100 episode reward | 4.4      |
| steps                   | 823995   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2700     |
| mean 100 episode reward | 4.8      |
| steps                   | 865392   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2800     |
| mean 100 episode reward | 5.4      |
| steps                   | 905714   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 2900     |
| mean 100 episode reward | 4.1      |
| steps                   | 945704   |
--------------------------------------
--------------------------------------
| % time spent exploring  | 1        |
| episodes                | 3000     |
| mean 100 episode reward | 5.3      |
| steps                   | 984814   |
--------------------------------------
Restored model with mean reward: 5.5
